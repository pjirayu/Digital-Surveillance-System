<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0124)https://openaccess.thecvf.com/content_CVPR_2019/html/Qin_BASNet_Boundary-Aware_Salient_Object_Detection_CVPR_2019_paper.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<meta name="viewport" content="width=device-width, initial-scale=1">
<title>CVPR 2019 Open Access Repository</title>
<link rel="stylesheet" type="text/css" href="./CVPR 2019 Open Access Repository_files/conf.css">
<script type="text/javascript" src="./CVPR 2019 Open Access Repository_files/jquery.js.download"></script>
<meta name="citation_title" content="BASNet: Boundary-Aware Salient Object Detection">
<meta name="citation_author" content="Qin, Xuebin">
<meta name="citation_author" content="Zhang, Zichen">
<meta name="citation_author" content="Huang, Chenyang">
<meta name="citation_author" content="Gao, Chao">
<meta name="citation_author" content="Dehghan, Masood">
<meta name="citation_author" content="Jagersand, Martin">
<meta name="citation_publication_date" content="2019">
<meta name="citation_conference_title" content="Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition">
<meta name="citation_firstpage" content="7479">
<meta name="citation_lastpage" content="7489">
<meta name="citation_pdf_url" content="http://openaccess.thecvf.com/content_CVPR_2019/papers/Qin_BASNet_Boundary-Aware_Salient_Object_Detection_CVPR_2019_paper.pdf">
</head>
<body>
<div id="header">
<div id="header_left">
<a href="http://cvpr2019.thecvf.com/"><img src="./CVPR 2019 Open Access Repository_files/cvpr2019_logo.png" width="175" border="0" alt="CVPR 2019"></a>
<a href="http://www.cv-foundation.org/"><img src="./CVPR 2019 Open Access Repository_files/cropped-cvf-s.jpg" width="175" height="112" border="0" alt="CVF"></a>
</div>
<div id="header_right">
<div id="header_title">
<a href="http://cvpr2019.thecvf.com/">CVPR 2019</a> <a href="https://openaccess.thecvf.com/" class="a_monochrome">open access</a>
</div>
<div id="help">
These CVPR 2019 papers are the Open Access versions, provided by the <a href="http://www.cv-foundation.org/">Computer Vision Foundation.</a><br> Except for the watermark, they are identical to the accepted versions; the final published version of the proceedings is available on IEEE Xplore.</div>
<div id="disclaimer">
This material is presented to ensure timely dissemination of scholarly and technical work.
Copyright and all rights therein are retained by authors or by other copyright holders.
All persons copying this information are expected to adhere to the terms and constraints invoked by each author's copyright.<br><br>
<form action="https://openaccess.thecvf.com/CVPR2019_search.py" method="post">
<input type="text" name="query">
<input type="submit" value="Search">
</form>
</div>
</div>
</div>
<div class="clear">
</div>
<div id="content">
<dl>
<dd>
<div id="papertitle">
BASNet: Boundary-Aware Salient Object Detection</div>
<div id="authors">
<br><b><i>Xuebin Qin,  Zichen Zhang,  Chenyang Huang,  Chao Gao,  Masood Dehghan,  Martin Jagersand</i></b>; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 7479-7489
</div><font size="5">
<br><b>Abstract</b>
</font>
<br><br><div id="abstract">
Deep Convolutional Neural Networks have been adopted for salient object detection and achieved the state-of-the-art performance. Most of the previous works however focus on region accuracy but not on the boundary quality. In this paper, we propose a predict-refine architecture, BASNet, and a new hybrid loss for Boundary-Aware Salient object detection. Specifically, the architecture is composed of a densely supervised Encoder-Decoder network and a residual refinement module, which are respectively in charge of saliency prediction and saliency map refinement. The hybrid loss guides the network to learn the transformation between the input image and the ground truth in a three-level hierarchy -- pixel-, patch- and map- level -- by fusing Binary Cross Entropy (BCE), Structural SIMilarity (SSIM) and Intersection-over-Union (IoU) losses. Equipped with the hybrid loss, the proposed predict-refine architecture is able to effectively segment the salient object regions and accurately predict the fine structures with clear boundaries. Experimental results on six public datasets show that our method outperforms the state-of-the-art methods both in terms of regional and boundary evaluation measures. Our method runs at over 25 fps on a single GPU. The code is available at: https://github.com/NathanUA/BASNet.</div>
<font size="5">
<br><b>Related Material</b>
</font>
<br><br>
[<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Qin_BASNet_Boundary-Aware_Salient_Object_Detection_CVPR_2019_paper.pdf">pdf</a>]
[<a href="https://openaccess.thecvf.com/content_CVPR_2019/supplemental/Qin_BASNet_Boundary-Aware_Salient_CVPR_2019_supplemental.pdf">supp</a>]
<div class="link2">[<a class="fakelink" onclick="$(this).siblings(&#39;.bibref&#39;).slideToggle()">bibtex</a>]
<div class="bibref">
@InProceedings{Qin_2019_CVPR,<br>
author = {Qin, Xuebin and Zhang, Zichen and Huang, Chenyang and Gao, Chao and Dehghan, Masood and Jagersand, Martin},<br>
title = {BASNet: Boundary-Aware Salient Object Detection},<br>
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
month = {June},<br>
year = {2019}<br>
}
</div>
</div>
</dd>
</dl>
</div>


</body></html>