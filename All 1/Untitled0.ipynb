{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"authorship_tag":"ABX9TyOhsLsXPEnqTyyHYkxvLP5R"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"KY4boMtTvq7O"},"source":["import torch\r\n","import torch.nn as nn\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","\r\n","# Hyper-parameters\r\n","num_epochs = 1000\r\n","learning_rate = 0.1\r\n","\r\n","# Toy dataset\r\n","x_train = torch.FloatTensor([[3.3, 1.7], [4.4, 2.76], [5.5, 2.09], [6.71, 3.19], [6.93, 1.694], [4.168, 1.573], \r\n","                    [9.779, 3.366], [6.182, 2.596], [7.59, 2.53], [2.167, 1.221], [7.042, 2.827], \r\n","                    [10.791, 3.465], [5.313, 1.65], [7.997, 2.904], [3.1, 1.3]])\r\n","\r\n","y_train = torch.LongTensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0])\r\n","\r\n","# Linear regression model\r\n","class Feedforward(torch.nn.Module):\r\n","        def __init__(self):\r\n","            super(Feedforward, self).__init__()\r\n","            self.input_size = 2\r\n","            self.hidden1_size = 4\r\n","            self.hidden2_size = 4\r\n","            self.output_size = 2\r\n","            self.fc1 = torch.nn.Linear(self.input_size, self.hidden1_size)\t\t\t#weight input-hidden1 layer \r\n","            self.relu1 = torch.nn.ReLU()\t\t\t\t\t\t\t\t\t\t\t#activation fn for #hidden layer 1\r\n","            self.fc2 = torch.nn.Linear(self.hidden1_size, self.hidden2_size)\t\t#weight hidden1-hidden2 layer\r\n","            self.relu2 = torch.nn.ReLU()\t\t\t\t\t\t\t\t\t\t\t#activation fn for #hidden layer 2\r\n","            self.fc3 = torch.nn.Linear(self.hidden2_size, self.output_size)\t\t\t#weight hidden2-output layer\r\n","            \r\n","\r\n","        def forward(self, x):\r\n","            hidden1 = self.fc1(x)\t\t\t\t\t#hidden layer 1\r\n","            relu1 = self.relu1(hidden1)\t\t\t\t#FF activation fn for #hidden layer 1\r\n","            hidden2 = self.fc2(relu1)\t\t\t\t#hidden layer 2\r\n","            relu2 = self.relu2(hidden2)\t\t\t\t#FF activation fn for #hidden layer 2\r\n","            output = self.fc3(relu2)\t\t\t\t#output layer\r\n","            return output\r\n","\r\n","def test(model, data, target):\r\n","    model.eval()\r\n","    correct = 0\r\n","    with torch.no_grad():\r\n","        output = model(data)\r\n","        pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\r\n","        correct += pred.eq(target.view_as(pred)).sum().item()\r\n","    print('Accuracy: {}/{} - {}%'.format(correct, len(pred), (correct/len(pred))*100))\r\n","\r\n","# Loss and optimizer\r\n","model=Feedforward()\r\n","print(model)\r\n","for p in model.parameters():\r\n","    p.data.fill_(0.02)\r\n","criterion = torch.nn.CrossEntropyLoss()\r\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \r\n","\r\n","# Train the model\r\n","for epoch in range(num_epochs):\r\n","    # Convert numpy arrays to torch tensors\r\n","    inputs = x_train\r\n","    targets = y_train\r\n","\r\n","    # Forward pass\r\n","    outputs = model(inputs)\r\n","    loss = criterion(outputs, targets)\r\n","    \r\n","    # Backward and optimize\r\n","    optimizer.zero_grad()\r\n","    loss.backward()\r\n","    optimizer.step()\r\n","    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\r\n","    test(model, inputs, targets)\r\n","\r\n","# Plot the graph\r\n","predicted = model(x_train).argmax(dim=1, keepdim=True).detach().numpy().flatten()\r\n","plt.subplot(1,2,1)\r\n","plt.title('Ground Truth')\r\n","plt.scatter(x_train[:,0], x_train[:,1], c=y_train)\r\n","plt.subplot(1,2,2)\r\n","plt.title('Predict')\r\n","plt.scatter(x_train[:,0], x_train[:,1], c=predicted)\r\n","plt.show()"],"execution_count":null,"outputs":[]}]}